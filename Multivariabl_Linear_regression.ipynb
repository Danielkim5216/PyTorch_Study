{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8387d41e-fb93-4fdf-af8b-f1b025abd789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2232afd1790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca4a2d3-81d2-430f-a2fe-6fb01a6f2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "# x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "# x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "# x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "# y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "#행렬 연산을 고려하여 구\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f770c6-ee30-402f-b81d-883e68a4fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "# w1 = torch.zeros(1, requires_grad=True)\n",
    "# w2 = torch.zeros(1, requires_grad=True)\n",
    "# w3 = torch.zeros(1, requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16b7871f-2e27-4e55-bcb1-b3e76b135bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 hypothesis: tensor([0.8338, 0.8338, 0.8338, 0.8338, 0.8338]) Cost: 29377.318359\n",
      "Epoch    1/200 hypothesis: tensor([67.2310, 80.6186, 76.5705, 86.4398, 61.6964]) Cost: 9446.208984\n",
      "Epoch    2/200 hypothesis: tensor([104.8735, 125.8509, 119.5085, 134.9721,  96.2013]) Cost: 3040.136230\n",
      "Epoch    3/200 hypothesis: tensor([126.2141, 151.4943, 143.8517, 162.4864, 115.7632]) Cost: 981.154602\n",
      "Epoch    4/200 hypothesis: tensor([138.3127, 166.0323, 157.6532, 178.0848, 126.8534]) Cost: 319.373688\n",
      "Epoch    5/200 hypothesis: tensor([145.1717, 174.2742, 165.4781, 186.9278, 133.1409]) Cost: 106.668823\n",
      "Epoch    6/200 hypothesis: tensor([149.0602, 178.9466, 169.9147, 191.9409, 136.7054]) Cost: 38.301476\n",
      "Epoch    7/200 hypothesis: tensor([151.2647, 181.5954, 172.4305, 194.7827, 138.7263]) Cost: 16.325787\n",
      "Epoch    8/200 hypothesis: tensor([152.5144, 183.0970, 173.8572, 196.3936, 139.8720]) Cost: 9.260714\n",
      "Epoch    9/200 hypothesis: tensor([153.2229, 183.9482, 174.6665, 197.3066, 140.5216]) Cost: 6.988255\n",
      "Epoch   10/200 hypothesis: tensor([153.6244, 184.4306, 175.1257, 197.8240, 140.8899]) Cost: 6.256101\n",
      "Epoch   11/200 hypothesis: tensor([153.8520, 184.7040, 175.3866, 198.1170, 141.0987]) Cost: 6.019073\n",
      "Epoch   12/200 hypothesis: tensor([153.9810, 184.8588, 175.5349, 198.2829, 141.2171]) Cost: 5.941185\n",
      "Epoch   13/200 hypothesis: tensor([154.0541, 184.9465, 175.6195, 198.3767, 141.2842]) Cost: 5.914413\n",
      "Epoch   14/200 hypothesis: tensor([154.0954, 184.9961, 175.6678, 198.4296, 141.3224]) Cost: 5.904120\n",
      "Epoch   15/200 hypothesis: tensor([154.1188, 185.0240, 175.6957, 198.4593, 141.3440]) Cost: 5.899056\n",
      "Epoch   16/200 hypothesis: tensor([154.1320, 185.0398, 175.7120, 198.4760, 141.3562]) Cost: 5.895698\n",
      "Epoch   17/200 hypothesis: tensor([154.1394, 185.0486, 175.7217, 198.4851, 141.3632]) Cost: 5.892934\n",
      "Epoch   18/200 hypothesis: tensor([154.1436, 185.0534, 175.7276, 198.4901, 141.3672]) Cost: 5.890311\n",
      "Epoch   19/200 hypothesis: tensor([154.1458, 185.0561, 175.7315, 198.4926, 141.3695]) Cost: 5.887766\n",
      "Epoch   20/200 hypothesis: tensor([154.1471, 185.0574, 175.7341, 198.4938, 141.3708]) Cost: 5.885248\n",
      "Epoch   21/200 hypothesis: tensor([154.1477, 185.0581, 175.7360, 198.4942, 141.3716]) Cost: 5.882688\n",
      "Epoch   22/200 hypothesis: tensor([154.1480, 185.0583, 175.7376, 198.4942, 141.3720]) Cost: 5.880186\n",
      "Epoch   23/200 hypothesis: tensor([154.1481, 185.0583, 175.7389, 198.4940, 141.3723]) Cost: 5.877674\n",
      "Epoch   24/200 hypothesis: tensor([154.1481, 185.0582, 175.7402, 198.4936, 141.3725]) Cost: 5.875150\n",
      "Epoch   25/200 hypothesis: tensor([154.1480, 185.0580, 175.7413, 198.4931, 141.3726]) Cost: 5.872638\n",
      "Epoch   26/200 hypothesis: tensor([154.1479, 185.0578, 175.7424, 198.4926, 141.3727]) Cost: 5.870099\n",
      "Epoch   27/200 hypothesis: tensor([154.1478, 185.0575, 175.7435, 198.4920, 141.3728]) Cost: 5.867590\n",
      "Epoch   28/200 hypothesis: tensor([154.1477, 185.0572, 175.7446, 198.4914, 141.3728]) Cost: 5.865054\n",
      "Epoch   29/200 hypothesis: tensor([154.1476, 185.0569, 175.7456, 198.4909, 141.3729]) Cost: 5.862545\n",
      "Epoch   30/200 hypothesis: tensor([154.1474, 185.0567, 175.7467, 198.4903, 141.3730]) Cost: 5.860036\n",
      "Epoch   31/200 hypothesis: tensor([154.1473, 185.0564, 175.7477, 198.4897, 141.3730]) Cost: 5.857518\n",
      "Epoch   32/200 hypothesis: tensor([154.1471, 185.0561, 175.7488, 198.4892, 141.3731]) Cost: 5.855044\n",
      "Epoch   33/200 hypothesis: tensor([154.1470, 185.0558, 175.7498, 198.4886, 141.3731]) Cost: 5.852499\n",
      "Epoch   34/200 hypothesis: tensor([154.1468, 185.0555, 175.7509, 198.4880, 141.3732]) Cost: 5.850002\n",
      "Epoch   35/200 hypothesis: tensor([154.1467, 185.0552, 175.7519, 198.4874, 141.3732]) Cost: 5.847501\n",
      "Epoch   36/200 hypothesis: tensor([154.1465, 185.0549, 175.7530, 198.4868, 141.3733]) Cost: 5.844982\n",
      "Epoch   37/200 hypothesis: tensor([154.1464, 185.0546, 175.7540, 198.4863, 141.3734]) Cost: 5.842493\n",
      "Epoch   38/200 hypothesis: tensor([154.1462, 185.0543, 175.7551, 198.4857, 141.3734]) Cost: 5.839996\n",
      "Epoch   39/200 hypothesis: tensor([154.1461, 185.0540, 175.7561, 198.4851, 141.3734]) Cost: 5.837505\n",
      "Epoch   40/200 hypothesis: tensor([154.1460, 185.0538, 175.7572, 198.4845, 141.3735]) Cost: 5.834990\n",
      "Epoch   41/200 hypothesis: tensor([154.1458, 185.0535, 175.7582, 198.4839, 141.3735]) Cost: 5.832478\n",
      "Epoch   42/200 hypothesis: tensor([154.1457, 185.0532, 175.7593, 198.4833, 141.3736]) Cost: 5.829978\n",
      "Epoch   43/200 hypothesis: tensor([154.1455, 185.0529, 175.7603, 198.4828, 141.3737]) Cost: 5.827477\n",
      "Epoch   44/200 hypothesis: tensor([154.1454, 185.0526, 175.7614, 198.4822, 141.3737]) Cost: 5.824990\n",
      "Epoch   45/200 hypothesis: tensor([154.1452, 185.0523, 175.7624, 198.4816, 141.3738]) Cost: 5.822494\n",
      "Epoch   46/200 hypothesis: tensor([154.1451, 185.0520, 175.7634, 198.4810, 141.3738]) Cost: 5.820012\n",
      "Epoch   47/200 hypothesis: tensor([154.1449, 185.0517, 175.7645, 198.4804, 141.3739]) Cost: 5.817530\n",
      "Epoch   48/200 hypothesis: tensor([154.1448, 185.0514, 175.7655, 198.4799, 141.3739]) Cost: 5.815019\n",
      "Epoch   49/200 hypothesis: tensor([154.1446, 185.0511, 175.7666, 198.4793, 141.3740]) Cost: 5.812525\n",
      "Epoch   50/200 hypothesis: tensor([154.1445, 185.0509, 175.7676, 198.4787, 141.3740]) Cost: 5.810054\n",
      "Epoch   51/200 hypothesis: tensor([154.1443, 185.0506, 175.7687, 198.4781, 141.3741]) Cost: 5.807546\n",
      "Epoch   52/200 hypothesis: tensor([154.1442, 185.0503, 175.7697, 198.4775, 141.3741]) Cost: 5.805067\n",
      "Epoch   53/200 hypothesis: tensor([154.1441, 185.0500, 175.7708, 198.4770, 141.3742]) Cost: 5.802585\n",
      "Epoch   54/200 hypothesis: tensor([154.1439, 185.0497, 175.7718, 198.4764, 141.3742]) Cost: 5.800094\n",
      "Epoch   55/200 hypothesis: tensor([154.1438, 185.0494, 175.7729, 198.4758, 141.3743]) Cost: 5.797588\n",
      "Epoch   56/200 hypothesis: tensor([154.1436, 185.0491, 175.7739, 198.4752, 141.3744]) Cost: 5.795123\n",
      "Epoch   57/200 hypothesis: tensor([154.1435, 185.0489, 175.7749, 198.4746, 141.3744]) Cost: 5.792632\n",
      "Epoch   58/200 hypothesis: tensor([154.1433, 185.0486, 175.7760, 198.4741, 141.3745]) Cost: 5.790154\n",
      "Epoch   59/200 hypothesis: tensor([154.1432, 185.0483, 175.7770, 198.4735, 141.3745]) Cost: 5.787698\n",
      "Epoch   60/200 hypothesis: tensor([154.1430, 185.0480, 175.7781, 198.4729, 141.3746]) Cost: 5.785216\n",
      "Epoch   61/200 hypothesis: tensor([154.1429, 185.0477, 175.7791, 198.4723, 141.3746]) Cost: 5.782725\n",
      "Epoch   62/200 hypothesis: tensor([154.1427, 185.0474, 175.7801, 198.4718, 141.3747]) Cost: 5.780279\n",
      "Epoch   63/200 hypothesis: tensor([154.1426, 185.0471, 175.7812, 198.4712, 141.3747]) Cost: 5.777795\n",
      "Epoch   64/200 hypothesis: tensor([154.1424, 185.0468, 175.7822, 198.4706, 141.3748]) Cost: 5.775323\n",
      "Epoch   65/200 hypothesis: tensor([154.1423, 185.0466, 175.7833, 198.4700, 141.3748]) Cost: 5.772849\n",
      "Epoch   66/200 hypothesis: tensor([154.1421, 185.0462, 175.7843, 198.4694, 141.3749]) Cost: 5.770374\n",
      "Epoch   67/200 hypothesis: tensor([154.1420, 185.0460, 175.7854, 198.4689, 141.3749]) Cost: 5.767892\n",
      "Epoch   68/200 hypothesis: tensor([154.1418, 185.0457, 175.7864, 198.4683, 141.3750]) Cost: 5.765423\n",
      "Epoch   69/200 hypothesis: tensor([154.1417, 185.0454, 175.7874, 198.4677, 141.3750]) Cost: 5.762979\n",
      "Epoch   70/200 hypothesis: tensor([154.1416, 185.0451, 175.7885, 198.4671, 141.3751]) Cost: 5.760492\n",
      "Epoch   71/200 hypothesis: tensor([154.1414, 185.0448, 175.7895, 198.4665, 141.3752]) Cost: 5.758025\n",
      "Epoch   72/200 hypothesis: tensor([154.1413, 185.0445, 175.7905, 198.4660, 141.3752]) Cost: 5.755570\n",
      "Epoch   73/200 hypothesis: tensor([154.1411, 185.0443, 175.7916, 198.4654, 141.3753]) Cost: 5.753128\n",
      "Epoch   74/200 hypothesis: tensor([154.1410, 185.0440, 175.7926, 198.4648, 141.3753]) Cost: 5.750663\n",
      "Epoch   75/200 hypothesis: tensor([154.1408, 185.0437, 175.7937, 198.4642, 141.3754]) Cost: 5.748183\n",
      "Epoch   76/200 hypothesis: tensor([154.1407, 185.0434, 175.7947, 198.4637, 141.3754]) Cost: 5.745743\n",
      "Epoch   77/200 hypothesis: tensor([154.1405, 185.0431, 175.7957, 198.4631, 141.3755]) Cost: 5.743275\n",
      "Epoch   78/200 hypothesis: tensor([154.1404, 185.0428, 175.7968, 198.4625, 141.3755]) Cost: 5.740838\n",
      "Epoch   79/200 hypothesis: tensor([154.1402, 185.0425, 175.7978, 198.4619, 141.3756]) Cost: 5.738400\n",
      "Epoch   80/200 hypothesis: tensor([154.1401, 185.0423, 175.7988, 198.4614, 141.3757]) Cost: 5.735948\n",
      "Epoch   81/200 hypothesis: tensor([154.1399, 185.0420, 175.7999, 198.4608, 141.3757]) Cost: 5.733499\n",
      "Epoch   82/200 hypothesis: tensor([154.1398, 185.0417, 175.8009, 198.4602, 141.3758]) Cost: 5.731036\n",
      "Epoch   83/200 hypothesis: tensor([154.1396, 185.0414, 175.8019, 198.4597, 141.3758]) Cost: 5.728602\n",
      "Epoch   84/200 hypothesis: tensor([154.1395, 185.0411, 175.8030, 198.4591, 141.3759]) Cost: 5.726140\n",
      "Epoch   85/200 hypothesis: tensor([154.1393, 185.0408, 175.8040, 198.4585, 141.3759]) Cost: 5.723680\n",
      "Epoch   86/200 hypothesis: tensor([154.1392, 185.0405, 175.8050, 198.4579, 141.3760]) Cost: 5.721248\n",
      "Epoch   87/200 hypothesis: tensor([154.1391, 185.0403, 175.8061, 198.4573, 141.3760]) Cost: 5.718800\n",
      "Epoch   88/200 hypothesis: tensor([154.1389, 185.0400, 175.8071, 198.4568, 141.3761]) Cost: 5.716367\n",
      "Epoch   89/200 hypothesis: tensor([154.1388, 185.0397, 175.8082, 198.4562, 141.3762]) Cost: 5.713922\n",
      "Epoch   90/200 hypothesis: tensor([154.1386, 185.0394, 175.8092, 198.4556, 141.3762]) Cost: 5.711492\n",
      "Epoch   91/200 hypothesis: tensor([154.1385, 185.0391, 175.8102, 198.4550, 141.3763]) Cost: 5.709033\n",
      "Epoch   92/200 hypothesis: tensor([154.1383, 185.0388, 175.8112, 198.4545, 141.3763]) Cost: 5.706604\n",
      "Epoch   93/200 hypothesis: tensor([154.1382, 185.0386, 175.8123, 198.4539, 141.3764]) Cost: 5.704173\n",
      "Epoch   94/200 hypothesis: tensor([154.1380, 185.0383, 175.8133, 198.4533, 141.3764]) Cost: 5.701772\n",
      "Epoch   95/200 hypothesis: tensor([154.1379, 185.0380, 175.8143, 198.4528, 141.3765]) Cost: 5.699306\n",
      "Epoch   96/200 hypothesis: tensor([154.1377, 185.0377, 175.8154, 198.4522, 141.3765]) Cost: 5.696839\n",
      "Epoch   97/200 hypothesis: tensor([154.1376, 185.0374, 175.8164, 198.4516, 141.3766]) Cost: 5.694438\n",
      "Epoch   98/200 hypothesis: tensor([154.1374, 185.0371, 175.8174, 198.4510, 141.3766]) Cost: 5.692023\n",
      "Epoch   99/200 hypothesis: tensor([154.1373, 185.0369, 175.8185, 198.4505, 141.3767]) Cost: 5.689584\n",
      "Epoch  100/200 hypothesis: tensor([154.1371, 185.0366, 175.8195, 198.4499, 141.3768]) Cost: 5.687130\n",
      "Epoch  101/200 hypothesis: tensor([154.1370, 185.0363, 175.8205, 198.4493, 141.3768]) Cost: 5.684706\n",
      "Epoch  102/200 hypothesis: tensor([154.1368, 185.0360, 175.8215, 198.4487, 141.3769]) Cost: 5.682283\n",
      "Epoch  103/200 hypothesis: tensor([154.1367, 185.0357, 175.8226, 198.4482, 141.3769]) Cost: 5.679872\n",
      "Epoch  104/200 hypothesis: tensor([154.1365, 185.0354, 175.8236, 198.4476, 141.3770]) Cost: 5.677474\n",
      "Epoch  105/200 hypothesis: tensor([154.1364, 185.0352, 175.8246, 198.4470, 141.3770]) Cost: 5.675028\n",
      "Epoch  106/200 hypothesis: tensor([154.1362, 185.0349, 175.8257, 198.4465, 141.3771]) Cost: 5.672587\n",
      "Epoch  107/200 hypothesis: tensor([154.1361, 185.0346, 175.8267, 198.4459, 141.3772]) Cost: 5.670197\n",
      "Epoch  108/200 hypothesis: tensor([154.1359, 185.0343, 175.8277, 198.4453, 141.3772]) Cost: 5.667748\n",
      "Epoch  109/200 hypothesis: tensor([154.1358, 185.0341, 175.8288, 198.4447, 141.3773]) Cost: 5.665327\n",
      "Epoch  110/200 hypothesis: tensor([154.1357, 185.0338, 175.8298, 198.4442, 141.3773]) Cost: 5.662921\n",
      "Epoch  111/200 hypothesis: tensor([154.1355, 185.0335, 175.8308, 198.4436, 141.3774]) Cost: 5.660491\n",
      "Epoch  112/200 hypothesis: tensor([154.1353, 185.0332, 175.8318, 198.4430, 141.3774]) Cost: 5.658125\n",
      "Epoch  113/200 hypothesis: tensor([154.1352, 185.0329, 175.8328, 198.4425, 141.3775]) Cost: 5.655697\n",
      "Epoch  114/200 hypothesis: tensor([154.1351, 185.0326, 175.8339, 198.4419, 141.3776]) Cost: 5.653261\n",
      "Epoch  115/200 hypothesis: tensor([154.1349, 185.0323, 175.8349, 198.4413, 141.3776]) Cost: 5.650887\n",
      "Epoch  116/200 hypothesis: tensor([154.1348, 185.0321, 175.8359, 198.4408, 141.3777]) Cost: 5.648458\n",
      "Epoch  117/200 hypothesis: tensor([154.1346, 185.0318, 175.8369, 198.4402, 141.3777]) Cost: 5.646040\n",
      "Epoch  118/200 hypothesis: tensor([154.1344, 185.0315, 175.8380, 198.4396, 141.3778]) Cost: 5.643637\n",
      "Epoch  119/200 hypothesis: tensor([154.1343, 185.0312, 175.8390, 198.4390, 141.3779]) Cost: 5.641249\n",
      "Epoch  120/200 hypothesis: tensor([154.1342, 185.0310, 175.8400, 198.4385, 141.3779]) Cost: 5.638822\n",
      "Epoch  121/200 hypothesis: tensor([154.1340, 185.0307, 175.8410, 198.4379, 141.3780]) Cost: 5.636439\n",
      "Epoch  122/200 hypothesis: tensor([154.1339, 185.0304, 175.8421, 198.4373, 141.3780]) Cost: 5.634049\n",
      "Epoch  123/200 hypothesis: tensor([154.1337, 185.0301, 175.8431, 198.4368, 141.3781]) Cost: 5.631650\n",
      "Epoch  124/200 hypothesis: tensor([154.1336, 185.0298, 175.8441, 198.4362, 141.3781]) Cost: 5.629211\n",
      "Epoch  125/200 hypothesis: tensor([154.1334, 185.0296, 175.8451, 198.4356, 141.3782]) Cost: 5.626842\n",
      "Epoch  126/200 hypothesis: tensor([154.1333, 185.0293, 175.8462, 198.4350, 141.3783]) Cost: 5.624417\n",
      "Epoch  127/200 hypothesis: tensor([154.1331, 185.0290, 175.8472, 198.4345, 141.3783]) Cost: 5.622035\n",
      "Epoch  128/200 hypothesis: tensor([154.1330, 185.0287, 175.8482, 198.4339, 141.3784]) Cost: 5.619609\n",
      "Epoch  129/200 hypothesis: tensor([154.1328, 185.0284, 175.8492, 198.4333, 141.3784]) Cost: 5.617258\n",
      "Epoch  130/200 hypothesis: tensor([154.1327, 185.0282, 175.8503, 198.4328, 141.3785]) Cost: 5.614835\n",
      "Epoch  131/200 hypothesis: tensor([154.1325, 185.0279, 175.8513, 198.4322, 141.3785]) Cost: 5.612453\n",
      "Epoch  132/200 hypothesis: tensor([154.1324, 185.0276, 175.8523, 198.4316, 141.3786]) Cost: 5.610083\n",
      "Epoch  133/200 hypothesis: tensor([154.1322, 185.0273, 175.8533, 198.4311, 141.3787]) Cost: 5.607664\n",
      "Epoch  134/200 hypothesis: tensor([154.1321, 185.0271, 175.8543, 198.4305, 141.3787]) Cost: 5.605299\n",
      "Epoch  135/200 hypothesis: tensor([154.1319, 185.0268, 175.8553, 198.4299, 141.3788]) Cost: 5.602921\n",
      "Epoch  136/200 hypothesis: tensor([154.1318, 185.0265, 175.8564, 198.4294, 141.3788]) Cost: 5.600514\n",
      "Epoch  137/200 hypothesis: tensor([154.1316, 185.0262, 175.8574, 198.4288, 141.3789]) Cost: 5.598164\n",
      "Epoch  138/200 hypothesis: tensor([154.1315, 185.0259, 175.8584, 198.4282, 141.3789]) Cost: 5.595762\n",
      "Epoch  139/200 hypothesis: tensor([154.1313, 185.0257, 175.8594, 198.4277, 141.3790]) Cost: 5.593388\n",
      "Epoch  140/200 hypothesis: tensor([154.1312, 185.0254, 175.8604, 198.4271, 141.3791]) Cost: 5.590996\n",
      "Epoch  141/200 hypothesis: tensor([154.1310, 185.0251, 175.8615, 198.4265, 141.3791]) Cost: 5.588578\n",
      "Epoch  142/200 hypothesis: tensor([154.1309, 185.0248, 175.8625, 198.4259, 141.3792]) Cost: 5.586214\n",
      "Epoch  143/200 hypothesis: tensor([154.1307, 185.0246, 175.8635, 198.4254, 141.3792]) Cost: 5.583856\n",
      "Epoch  144/200 hypothesis: tensor([154.1306, 185.0243, 175.8645, 198.4248, 141.3793]) Cost: 5.581484\n",
      "Epoch  145/200 hypothesis: tensor([154.1304, 185.0240, 175.8655, 198.4243, 141.3793]) Cost: 5.579101\n",
      "Epoch  146/200 hypothesis: tensor([154.1303, 185.0237, 175.8665, 198.4237, 141.3794]) Cost: 5.576722\n",
      "Epoch  147/200 hypothesis: tensor([154.1301, 185.0235, 175.8675, 198.4231, 141.3795]) Cost: 5.574351\n",
      "Epoch  148/200 hypothesis: tensor([154.1300, 185.0232, 175.8685, 198.4226, 141.3795]) Cost: 5.572006\n",
      "Epoch  149/200 hypothesis: tensor([154.1298, 185.0229, 175.8696, 198.4220, 141.3796]) Cost: 5.569607\n",
      "Epoch  150/200 hypothesis: tensor([154.1297, 185.0226, 175.8706, 198.4214, 141.3797]) Cost: 5.567238\n",
      "Epoch  151/200 hypothesis: tensor([154.1295, 185.0223, 175.8716, 198.4209, 141.3797]) Cost: 5.564883\n",
      "Epoch  152/200 hypothesis: tensor([154.1294, 185.0220, 175.8726, 198.4203, 141.3798]) Cost: 5.562510\n",
      "Epoch  153/200 hypothesis: tensor([154.1292, 185.0218, 175.8736, 198.4197, 141.3798]) Cost: 5.560170\n",
      "Epoch  154/200 hypothesis: tensor([154.1291, 185.0215, 175.8746, 198.4192, 141.3799]) Cost: 5.557763\n",
      "Epoch  155/200 hypothesis: tensor([154.1289, 185.0212, 175.8757, 198.4186, 141.3799]) Cost: 5.555421\n",
      "Epoch  156/200 hypothesis: tensor([154.1288, 185.0210, 175.8767, 198.4180, 141.3800]) Cost: 5.553038\n",
      "Epoch  157/200 hypothesis: tensor([154.1286, 185.0207, 175.8777, 198.4175, 141.3801]) Cost: 5.550687\n",
      "Epoch  158/200 hypothesis: tensor([154.1285, 185.0204, 175.8787, 198.4169, 141.3801]) Cost: 5.548322\n",
      "Epoch  159/200 hypothesis: tensor([154.1283, 185.0201, 175.8797, 198.4163, 141.3802]) Cost: 5.545955\n",
      "Epoch  160/200 hypothesis: tensor([154.1282, 185.0199, 175.8807, 198.4158, 141.3802]) Cost: 5.543632\n",
      "Epoch  161/200 hypothesis: tensor([154.1280, 185.0196, 175.8817, 198.4152, 141.3803]) Cost: 5.541267\n",
      "Epoch  162/200 hypothesis: tensor([154.1279, 185.0193, 175.8827, 198.4147, 141.3804]) Cost: 5.538930\n",
      "Epoch  163/200 hypothesis: tensor([154.1277, 185.0191, 175.8838, 198.4141, 141.3804]) Cost: 5.536564\n",
      "Epoch  164/200 hypothesis: tensor([154.1276, 185.0188, 175.8848, 198.4135, 141.3805]) Cost: 5.534186\n",
      "Epoch  165/200 hypothesis: tensor([154.1274, 185.0185, 175.8858, 198.4130, 141.3805]) Cost: 5.531856\n",
      "Epoch  166/200 hypothesis: tensor([154.1273, 185.0182, 175.8868, 198.4124, 141.3806]) Cost: 5.529501\n",
      "Epoch  167/200 hypothesis: tensor([154.1271, 185.0179, 175.8878, 198.4118, 141.3807]) Cost: 5.527156\n",
      "Epoch  168/200 hypothesis: tensor([154.1270, 185.0177, 175.8888, 198.4112, 141.3807]) Cost: 5.524781\n",
      "Epoch  169/200 hypothesis: tensor([154.1268, 185.0174, 175.8898, 198.4107, 141.3808]) Cost: 5.522463\n",
      "Epoch  170/200 hypothesis: tensor([154.1267, 185.0171, 175.8908, 198.4101, 141.3808]) Cost: 5.520114\n",
      "Epoch  171/200 hypothesis: tensor([154.1265, 185.0169, 175.8918, 198.4096, 141.3809]) Cost: 5.517744\n",
      "Epoch  172/200 hypothesis: tensor([154.1264, 185.0166, 175.8928, 198.4090, 141.3810]) Cost: 5.515423\n",
      "Epoch  173/200 hypothesis: tensor([154.1262, 185.0163, 175.8938, 198.4084, 141.3810]) Cost: 5.513057\n",
      "Epoch  174/200 hypothesis: tensor([154.1261, 185.0160, 175.8949, 198.4079, 141.3811]) Cost: 5.510712\n",
      "Epoch  175/200 hypothesis: tensor([154.1259, 185.0157, 175.8959, 198.4073, 141.3811]) Cost: 5.508383\n",
      "Epoch  176/200 hypothesis: tensor([154.1258, 185.0155, 175.8969, 198.4067, 141.3812]) Cost: 5.506034\n",
      "Epoch  177/200 hypothesis: tensor([154.1256, 185.0152, 175.8979, 198.4062, 141.3813]) Cost: 5.503710\n",
      "Epoch  178/200 hypothesis: tensor([154.1255, 185.0149, 175.8989, 198.4056, 141.3813]) Cost: 5.501325\n",
      "Epoch  179/200 hypothesis: tensor([154.1253, 185.0147, 175.8999, 198.4051, 141.3814]) Cost: 5.499036\n",
      "Epoch  180/200 hypothesis: tensor([154.1252, 185.0144, 175.9009, 198.4045, 141.3814]) Cost: 5.496713\n",
      "Epoch  181/200 hypothesis: tensor([154.1250, 185.0141, 175.9019, 198.4039, 141.3815]) Cost: 5.494360\n",
      "Epoch  182/200 hypothesis: tensor([154.1249, 185.0139, 175.9029, 198.4034, 141.3816]) Cost: 5.492033\n",
      "Epoch  183/200 hypothesis: tensor([154.1247, 185.0136, 175.9039, 198.4028, 141.3816]) Cost: 5.489734\n",
      "Epoch  184/200 hypothesis: tensor([154.1246, 185.0133, 175.9049, 198.4023, 141.3817]) Cost: 5.487394\n",
      "Epoch  185/200 hypothesis: tensor([154.1244, 185.0130, 175.9059, 198.4017, 141.3818]) Cost: 5.485056\n",
      "Epoch  186/200 hypothesis: tensor([154.1243, 185.0128, 175.9069, 198.4011, 141.3818]) Cost: 5.482716\n",
      "Epoch  187/200 hypothesis: tensor([154.1241, 185.0125, 175.9079, 198.4006, 141.3819]) Cost: 5.480404\n",
      "Epoch  188/200 hypothesis: tensor([154.1240, 185.0122, 175.9089, 198.4000, 141.3819]) Cost: 5.478080\n",
      "Epoch  189/200 hypothesis: tensor([154.1238, 185.0119, 175.9099, 198.3994, 141.3820]) Cost: 5.475736\n",
      "Epoch  190/200 hypothesis: tensor([154.1237, 185.0117, 175.9109, 198.3989, 141.3820]) Cost: 5.473425\n",
      "Epoch  191/200 hypothesis: tensor([154.1235, 185.0114, 175.9119, 198.3983, 141.3821]) Cost: 5.471080\n",
      "Epoch  192/200 hypothesis: tensor([154.1234, 185.0111, 175.9129, 198.3978, 141.3822]) Cost: 5.468781\n",
      "Epoch  193/200 hypothesis: tensor([154.1232, 185.0109, 175.9139, 198.3972, 141.3822]) Cost: 5.466463\n",
      "Epoch  194/200 hypothesis: tensor([154.1231, 185.0106, 175.9149, 198.3967, 141.3823]) Cost: 5.464158\n",
      "Epoch  195/200 hypothesis: tensor([154.1229, 185.0103, 175.9159, 198.3961, 141.3824]) Cost: 5.461839\n",
      "Epoch  196/200 hypothesis: tensor([154.1228, 185.0100, 175.9169, 198.3955, 141.3824]) Cost: 5.459518\n",
      "Epoch  197/200 hypothesis: tensor([154.1226, 185.0098, 175.9179, 198.3949, 141.3825]) Cost: 5.457172\n",
      "Epoch  198/200 hypothesis: tensor([154.1225, 185.0095, 175.9189, 198.3944, 141.3826]) Cost: 5.454877\n",
      "Epoch  199/200 hypothesis: tensor([154.1223, 185.0092, 175.9199, 198.3938, 141.3826]) Cost: 5.452562\n",
      "Epoch  200/200 hypothesis: tensor([154.1222, 185.0090, 175.9209, 198.3933, 141.3827]) Cost: 5.450260\n"
     ]
    }
   ],
   "source": [
    "#optimizer set \n",
    "#optimizer = optim.SGD([w1,w2,w3,b],lr=1e-5)\n",
    "optimizer = optim.SGD([W,b],lr=1e-5)\n",
    "epochs = 200\n",
    "for epoch in range(epochs + 1):\n",
    "    #H(x) 계산\n",
    "    #hypothesis = x1_train * w1 + x2_train * w2 + x3_train* w3 +b \n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    loss = torch.mean((hypothesis - y_train)**2)\n",
    "    #loss 계산\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "    #         epoch, epochs, w1.item(), w2.item(), w3.item(), b.item(), loss.item()\n",
    "    #     ))\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, epochs, hypothesis.squeeze().detach(), loss.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd3d6020-b94b-41b2-ae96-9ca74557658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for input [75.0, 85.0, 72.0]: 156.984619140625\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 값에 대한 예측\n",
    "with torch.no_grad():\n",
    "    new_input = torch.FloatTensor([[75, 85, 72]])  # 예측하고 싶은 임의의 입력\n",
    "    prediction = new_input.matmul(W) + b\n",
    "    print('Predicted value for input {}: {}'.format(new_input.squeeze().tolist(), prediction.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02784e-7eed-402e-ada4-f1c31d68a4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
